В отличие от традиционного машинного обучения, глубокое обучение не требует извлечения признаков в ручную. Это означает, что экспертам не нужно определять и выбирать наиболее важные признаки для модели глубокого обучения (==однако как традиционное машинное, так и глубокое обучение для классификации спама по-прежнему требуют присвоения меток, таких как "спам" или "не спам", которые должны быть присвоены либо экспертом, либо пользователем==) [[Список литературы#^8682c4]]<c. 25>.

LLM можно использовать для эффективного извлечения данных из больших объемов текста в специализированных областях, таких как медицина. Модели помогут отыскать нужные сведения в документах, обобщить длинные отрывки текста и найти ответы на технические вопросы.

Процесс обучения модели состоит из двух этапов:
- предварительного обучения: модель обучается на большой и разнообразной выборке данных для широкого понимания языка, 
- и тонкой настройки. 

Предварительно обученная модель затем служит в качестве ==базового ресурса==, который может быть дополнительно усовершенствован с помощью _тонкой настройки_. Это процесс, при котором модель обучается на меньшей выборке данных, более специфичной для конкретных задач или областей.

Предварительное обучение LLM (==без меток!==) включает в себя _предсказание следующего слова_ на больших текстовых выборках данных. Предварительно обученную модель затем можно точно настроить с помощью меньшей по размеру _размеченной выборки данных_ [[Список литературы#^8682c4]]<c. 28>

NB! Для _предварительного обучения_ модели разметка ==не требуется==. На данном этапе LLM использует самообучение, при котором модель сама генерирует собственные метки на основе входных данных. Таким образом, на этапе предварительного обучения создается LLM, которую часто называют базовой. Типичный пример -- GPT-3 [[Список литературы#^8682c4]]<c. 28>

Когда базовая LLM создана, можно переходить ко второму этапу -- _тонкой (или точной) настройке модели_. Здесь модель обучается на меньшей по размеру размеченной выборке данных, более характерной для конкретных задач или предметной области.

Существует две наиболее популярные категории тонкой настройки LLM [[Список литературы#^8682c4]]<c. 29>:
- При _тонкой настройке по инструкциям_ размеченная выборка данных состоит из пар "инструкция -- ответ", например фрагмент текста на языке оригинале и его правильный перевод на другой язык.
- При _тонкой настройке по классификации_ размеченная выборка данных состоит из текстов и связанных с ними меток классов, например электронных писем с метками "спам" и "не спам" 

Архитектура трансформера состоит из двух подмодулей:
- кодировщика
- и декодировщика.

Модуль кодировщика обрабатывает входной текст и кодирует его в виде набора числовых векторов, которые фиксируют контекстную информацию. Затем модуль декодировщика принимает эти векторы и генерирует выходной текст. Например, в задаче перевода кодировщик преобразует текст на исходном языке в векторы, а декодировщик преобразует их в текст на целевом языке. 

Предварительное обучение LLM требует доступа к значительным объемам вычислительных ресурсов и обходится очень дорого. Например, стоимость предварительного обучения GPT-3 оценивается в 4.6 млрд. долларов в расценках для облачных вычислений.

Хорошая новость -- многие предварительно обученные LLM, доступные как модели с исходным кодом, могут использоваться в качестве инструментов общего назначения для написания, извлечения и редактирования текстов, которые не были частью обучающих данных.

_Задача предсказания следующего слова_ является формой самообучения, то есть формой самостоятельной разметки данных. Это означает, что нам не нужно собирать метки для обучающих данных, а можно задействовать структуру самих данных: следующее слово в предложении можно использовать в качестве метки, которую должна предсказывать модель. Эта задача позволяет создавать метки динамически, так что для обучения LLM можно использовать большие массивы текстовых данных ==без меток== [[Список литературы#^8682c4]]<c. 34>.  

Общая архитектура GPT относительно проста. По сути, это просто декодировщик без кодировщика. Модели в стиле декодировщика, такие как GPT, генерируют текст, предсказывая его по одному слову за раз, поэтому считаются разновидностью _авторегрессионных моделей_ [[Список литературы#^8682c4]]<c. 35>.
## Векторные представления слов

Word2Vec обучает архитектуру нейронной сети генерировать векторные представления слов, предсказывая контекст слова по целевому слову или наоборот.

Основная идея Word2Vec заключается в том, что слова, которые появляются в схожих контекстах, как правило, имеют схожие значения. Следовательно, при проецировании на двумерные векторные представления слов в целях визуализации похожие термины группируются.

Векторные представления слов могут иметь разную длину или размерность, от одной до нескольких тысяч. Более высокая размерность может выявить более специфичные взаимосвязи, но за счет снижения вычислительной эффективности.

Можно использовать предварительно обученные модели, такие как Word2Vec, для создания векторных представлений, однако LLM обычно создают собственные представления, которые являются частью входного слоя и обновляются во время обучения. Векторные представления, оптимизированные в рамках обучения LLM, имеют преимущество по сравнению с использованием Word2Vec: они изменены под конкретную задачу и имеющиеся данные. 

NB! Длину вектора эммбидинга еще называют размерностью скрытых состояний модели  [[Список литературы#^8682c4]]<c. 44>

Длина вектора эммбединга это компромисс между производительностью и эффективностью. В самых маленьких моделях GPT-2 (117 и 125 млн параметров) длина вектора эммбединга составляет 768 элементов. В самой большой модели GPT-3 (175 млрд параметров) длина такого вектора составляет 12_288 элементов.
## Токенизация текста

Разделение на токены -- этап предварительной обработки текста, необходимый для создания векторных представлений для LLM. Эти токены представляют собой либо отдельные слова, либо специальные символы, в том числе знаки препинания. 

Токенизатор, используемый для моделей GPT, не использует токен `<|unk|>` для слов, которые не входят в словарь. Вместо этого в моделях GPT задействуется токенизатор кодирования пар байтов, который разбивает слова на более мелкие составляющие.
## Кодирование пар байтов

Алгоритм кодирования пар байт реализован в библиотеке tiktoken (`uv add tiktoken`)
```python
import tiktoken

text = "..."
tokenizer = tiktoken.get_encoding("gpt2")
# получаем индексы токенов
idxs: list[int] = tokenizer.encode(text, allowed_special={"<|endoftext|>"})
# получаем токены по индексам
tokenizer.decode(idxs)
```

Когда алгоритм BPE встречает незнакомые слова, он разбивает их на индивидуальные буквы и их сочетания, которые имеются в существующем словаре [[Список литературы#^8682c4]]<c. 59>

Небольшие размеры пакетов требуют меньше памяти во время обучения, но приводят к обновлениям модели, содержащим ==больше шумов==. Как и в глубоком обучении, размер пакета является компромиссом и гиперпараметром, с которым можно экспериментировать при обучении LLM [[Список литературы#^8682c4]]<c. 65>.

NB! Размер пакета -- гиперпараметр

Последний шаг в подготовке входного текста для обучения LLM -- преобразование _идентификаторов токенов_ в _векторные представления_. В качестве предварительного действия мы должны инициализировать весовые коэффициенты вложения случайными значениями. Такая инициализация служит отправной точкой для процесса обучения LLM. 

Слой вложения -- это по сути, операция поиска, которая извлекает строки из весовой матрицы слоя вложения по индетификатору токена [[Список литературы#^8682c4]]<c. 69>
```python
vocab_size = 6
output_dim = 3
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
print(embedding_layer.weight) # выведет матрицу весов (6 x 3)
print(embedding_layer(torch.tensor([3]))) # выведет 4-ую строку матрицы весов для идентификатора токена 3
```

 










